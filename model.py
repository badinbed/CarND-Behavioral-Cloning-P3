import os
import csv
import cv2
import math
import argparse
import pickle
import numpy as np

from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt


def load_samples(folders):
""" Parse a set of camera images from one or more csv files

Parameters
----------
folders : list
	A list of folders containing a csv file and camera images
	as generated by the self driving car simulator

Returns
--------
list
	List of samples, each containing the 3 camera image paths 
	converted to relative and the steering angle

"""
	samples = []
	for folder in folders:
		with open(os.path.join(folder, 'driving_log.csv')) as csvfile:
			reader = csv.reader(csvfile)
			for line in reader:
				for indx in range(3):
					# convert to relative path
					line[indx] = os.path.join(folder, 'IMG', os.path.basename(line[indx]))#
				
				# convert steering angle to float
				line[3] = float(line[3])
				samples.append(line[:4])
	return samples
	
			
class SampleGenerator:
	""" A wrapper class to configure the sample generator """

	def __init__(self, 
				samples, 
				num_iterations = 3, 
				batch_size = 256, 
				use_side_cams = False, 
				angle_correction = 0.25, 
				augment=False,
				skip_range = 0.1,
				skip_prob = 1.0
				skip_decay = 1.0):
		"""
		Parameters
		----------
		samples : list
			A list of samples as returned by load_samples()
		num_iterations: int
			Number of iterations per epoch through the sample set. As images are augmented
			and only one camera image per sample is selected it makes sense to iterate through
			the samples more than once per epoch
		batch_size: int
			number of images per batch_size
		use_side_cams: bool
			True of left and right camera images shall be used randomly
		angle_correction: float
			correction will be added/subtracted for left and right images
		augment: bool
			True if images shall be augmented
		skip_range: float
			steering angles within this range may be skipped randomly
		skip_prob: float
			starting probability that an image with angle y skip_range may be skipped
		skip_decay: float
			base factor that reduces the skip probability with each epoch:
			skip if abs(angle) < skip_range and np.random.uniform() > skip_prob / (epoch * skip_decay)

		"""
		n_cam_imgs = 3
		self.samples = samples
		self.use_side_cams = use_side_cams
		self.angle_correction = (0, angle_correction, -angle_correction)
		self.augment = augment
		self.num_samples = len(samples)
		self.batch_size = batch_size
		self.num_steps = math.ceil(self.num_samples * num_iterations / self.batch_size)
		self.skip_range = skip_range
		self.skip_prob = skip_prob
		self.skip_decay = skip_decay
	
	def aug_brightness(self, img, rmin=0.5, rmax = 1.5):
		""" change brightness of an image randomly
		
		Generates a random factor between rmin and rmax
		and multiplies is to the brghtness of the image
		
		Parameters
		-----------
		img: cv2 image
		rmin: float
			minimum brightness scaling
		rmax: float
			maximum brightness scaling
			
		Returns
		--------
		cv2 image
		"""
		
		hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
		hsv_arr = np.array(hsv, dtype = np.float64)
		
		# apply random factor to value (brightness) between [rmin, rmax]
		hsv_arr[:,:,2] = hsv_arr[:,:,2] * np.random.uniform(rmin, rmax)
		hsv_arr[:,:,2][hsv_arr[:,:,2]>255]  = 255 #clamp
		hsv_arr = np.array(hsv_arr, dtype = np.uint8)
		gbr = cv2.cvtColor(hsv_arr,cv2.COLOR_HSV2BGR)
		return gbr
		
	def aug_translation(elf, img, angle, range_x = 50, range_y = 20, deg_per_pixel = 0.004):
		""" Translate the image by random offset and adjusts the steering angle
		
		When shifting horizontally the steering angle needs to be adjusted.
		A shift to the left results in a positive steering angle correction
		a shift to the right in a negative correction.
		The sides are padded with zero values which is similar to crop augmentation
		and helps generalizing as well
	
		Parameters
		-----------
		img: cv2 image
		angle: float
			unaugmented steering angle
		range_x: float
			maximum horizontal shift in pixel to either side
		range_y: float
			maximum vertical shift in pixel to either side
		deg_per_pixel: float
			correction per pixel in horizontal shift
			
		Returns
		--------
		cv2 image, float
			Returns the translated image and teh corrected steering angle	
		"""
		tx = np.random.uniform(-range_x, range_x)
		ty = np.random.uniform(-range_y, range_y)

		#adjust steering angle according the horizontal shift
		angle += tx * angle_per_pixel
		
		# apply translation matrix
		M = np.float32([[1,0,tx],[0,1,ty]])
		rows, cols = img.shape[:2]
		translated = cv2.warpAffine(img, M , (cols, rows))
		return translated, angle	
	
			
	def augment(self, img, angle):
		""" Augments an image and adjust the steering angle accordingly
		
		Used augmentation techniques are
			- random translation and edge cropping
			- random brightness scaling
			- random flipping
			
		Parameters
		-----------
		img: cv2 image
		angle: float
		
		Returns
		--------
		cv2 image, float
			The augmented image and corrected angle
		"""
		img, 
		angle = self.aug_translation(img, angle, 50, 20, 0.004)
		img = self.aug_brightness(img, 0.5, 1.5)
		
		if np.random.uniform() < 0.5:
			img = cv2.flip(img, 1)
			angle = -angle
			
		return img, angle
		
			
	def generate(self):
		""" Generated batches of randomly augmented images 
		
			The generator will randomly skip images with a small steering angle
			as the training data has a bias towards driving straight. The probability
			to skip an image is getting lower with each epoch.
			The generator also selects which camera to use randomly and then augments the 
			camera image
		"""
		epoch = 0
		while 1: # Loop forever so the generator never terminates
			shuffle(self.samples)
			epoch += 1
			skip = self.skip_prob / (epoch * self.skip_decay) 
			for i in range(self.num_steps):
				images = []
				angles = []
				while len(images) < self.batch_size:
					
					# find random sample
					sample = self.samples[np.random.randint(self.num_samples)]
					
					#sample may be skipped randomly if steering angle is small
					while abs(sample[3]) < self.skip_range and np.random.uniform() > skip:
						sample = self.samples[np.random.randint(self.num_samples)]
						
					# randomly choose one of the camera images	
					img_idx = np.random.randint(3) if self.use_side_cams else 0
					image = cv2.imread(sample[img_idx])
					angle = sample[3] + self.angle_correction[img_idx]
					
					# augment the image
					if self.augment:
						image, angle = self.augment(image, angle)
					
					images.append(np.array(image))
					angles.append(angle)

				X_train = np.asarray(images)
				y_train = np.asarray(angles)
				
				yield X_train, y_train
				
				
	def test_augmentation(self, img_file, angle):
	""" Simple function to help display the augmentation methods
	"""
		result = []
		print('Testing augmentation on file:', img_file)
		img = cv2.imread(img_file)
		result.append((np.array(img), angle))
		result.append((np.array(cv2.flip(img, 1)), -angle))

		for rows in range(12):
			result.append((np.array(self.aug_brightness(img)), angle))
		for rows in range(12):
			i, a = self.aug_translation(img, angle)
			result.append((np.array(i), a))	
			
		print('augmentation result: ', len(result), 'images')
		return np.array(result)
		
			
from keras.models import Sequential, Model
from keras.layers import Flatten, Dense, Lambda, Cropping2D, Conv2D, Activation, AveragePooling2D
import keras

def create_nvidia_model():
	'''
	Generates a model based on nvidias approach using:
		- a normalization layer to shift values from [0, 255] to [-1, 1]
		- a cropping layer to cut of car hood and sky/trees
		- 3 convolutional layers with kernel=5x5 and stride=2x2
		- 2 convolutional layers with kernel=3x3 and stride=1x1
		- a flattening layer
		- 4 fully connected layers with 100, 50, 10, 1 outbound connections
		
		- Activation functions throughout are relu
		- loss function is mean squared error
		- Optimizer is adam
	'''
	model = Sequential()

    # attempt to resize model in a simple manner but didn't like the results
    #model.add(AveragePooling2D(input_shape=(160, 320, 3)))
    
	# Preprocess incoming data, centered around zero with small standard deviation 
	model.add(Lambda(lambda x: x/127.5 - 1.0, input_shape=(160, 320, 3)))

	# crop out top 70 rows (sky/trees) and bottom 25 (car hood)
	model.add(Cropping2D(cropping=((70, 25), (0, 0))))

	model.add(Conv2D(24, (5, 5), strides=(2, 2)))
	model.add(Activation('relu'))

	model.add(Conv2D(36, (5, 5), strides=(2, 2)))
	model.add(Activation('relu'))

	model.add(Conv2D(48, (5, 5), strides=(2, 2)))
	model.add(Activation('relu'))

	model.add(Conv2D(64, (3, 3)))
	model.add(Activation('relu'))

	model.add(Conv2D(64, (3, 3)))
	model.add(Activation('relu'))

	model.add(Flatten())
	model.add(Dense(100))
	model.add(Dense(50))
	model.add(Dense(10))
	model.add(Dense(1))
	model.compile(loss='mse', optimizer='adam')
	return model
 

def main():
	# command line arguments
	parser= argparse.ArgumentParser()
	parser.add_argument('-sf', '--sample_folder', nargs='+')
	parser.add_argument('--middle_only', default=False, action='store_true')
	parser.add_argument('-noaug', '--no_augmentation', default=False, action='store_true')
	parser.add_argument('-ac', '--angle_correction', default=0.25, type=float)
	parser.add_argument('-e', '--epochs', default=5, type=int)
	parser.add_argument('-bs', '--batch_size', default=256, type=int)
	parser.add_argument('-ss', '--sample_size', default=None, type=int)
	parser.add_argument('-vs', '--validation_size', default=0.2, type=float)
	parser.add_argument('-lm', '--load_model', default=None)
	parser.add_argument('-sm', '--save_model', default=None)
	parser.add_argument('-lr', '--learning_rate', default=0.001, type=float)
	parser.add_argument('-sr', '--skip_range', default=0.1, type=float)
	parser.add_argument('-sp', '--skip_prob', default=1.0, type=float)
	parser.add_argument('-sd', '--skip_decay', default=1.0, type=float)
	parser.add_argument('-a', '--action', default='train', choices=['train', 'eval', 'test_augment', 'plot_history'])

	args =  parser.parse_args()
    
	with keras.backend.get_session():
		# load samples for training or evaluation
		if args.sample_folder:
			samples = load_samples(args.sample_folder)
			if len(samples) > 0:
				samples = shuffle(samples, n_samples=args.sample_size)
				print("Samples loaded from {}: {}".format(args.sample_folder, len(samples)))
			else:
				print('No samples loaded')

		
		# load model or create a new one if none specified
		if not args.load_model:
			model = create_nvidia_model()
			history = {'loss':[], 'val_loss':[] , 'epochs':[], 'sources':[]}
			print('New model based on nvidia compiled using mse loss and adam optimizer')
			model.summary()
		else:
			model = keras.models.load_model(args.load_model)
			filename, extension = os.path.splitext(args.load_model)
			history = pickle.load(open(filename + '_hist.p', 'rb'))
			print('Loaded model:', args.load_model)
			model.summary()
		model.optimizer.lr.assign(args.learning_rate)
		
			
		# if we trained the model we want to save the result
		if args.action == 'train':	
			# split samples into training and validation 
			train_samples, validation_samples = train_test_split(samples, test_size=args.validation_size)
			
			# create a generator for training. 
			# This generator may augment and skip data according to its parameterss
			train_gen = SampleGenerator(train_samples, 
									batch_size=args.batch_size, 
									use_side_cams = not args.middle_only, 
									angle_correction = args.angle_correction,
									augment = not args.no_augmentation,
									skip_range = args.skip_range,
									skip_prob = args.skip_prob,
									skip_decay = args.skip_decay)
		
			# create generator for validation
			# This generator will not augment or skip data
			valid_gen = SampleGenerator(validation_samples, 
									batch_size=args.batch_size, 
									use_side_cams = not args.middle_only, 
									angle_correction=args.angle_correction,
									augment = False,
									skip_prob = 0.0)	
											
		
			print("Training model: epochs={}, batch_size={} steps_per_epoch={}, validation_steps={}".format(args.epochs, train_gen.batch_size, train_gen.num_steps, valid_gen.num_steps))			
			hist_object = model.fit_generator(train_gen.generate(), 
												steps_per_epoch = train_gen.num_steps, 
												validation_data = valid_gen.generate(), 
												validation_steps = valid_gen.num_steps, 
												epochs = args.epochs, 
												verbose=1)

			# accumulate with loaded history
			history['loss'] += hist_object.history['loss']
			history['val_loss'] += hist_object.history['val_loss']
			history['epochs'].append(args.epochs)
			history['sources'].append(args.sample_folder)
			
			# save model and history
			if not args.save_model:
				args.save_model = args.load_model
			print('Saving model to:', args.save_model)
			model.save(args.save_model)
			hist_file, extension = os.path.splitext(args.save_model)
			hist_file += '_hist.p'
			print('Saving history to:', hist_file)
			pickle.dump(history, open(hist_file, 'wb'))
				
		# evaluate model with given data		
		elif args.action == 'eval':
			eval_gen = SampleGenerator(samples, 
									batch_size = args.batch_size, 
									use_side_cams = not args.middle_only, 
									angle_correction=args.angle_correction)
			print("Evaluating model: batch_size={} steps={}".format(eval_gen.batch_size, eval_gen.num_steps))			
			
			loss = model.evaluate_generator(eval_gen.generate(), steps = eval_gen.num_steps)
			print('Average loss:', loss / len(samples)) # sucks that evaluate_generator doesnt have verbosity
			
		# demonstration of data augmentation methods	
		elif args.action == 'test_augment':
			gen = SampleGenerator(samples)
			images = gen.test_augmentation(samples[0][0], samples[0][3])
			
			for i in range(len(images)):
				file_name = 'test_augment-{:04}-{:4.3f}.jpg'.format(i, images[i][1])
				print(file_name)
				cv2.imwrite(os.path.join('augmentation', file_name), images[i][0])
				
		# plot history object
		elif args.action == 'plot_history':
			x = list(range(1, len(history['loss']) + 1))
			plt.plot(x, history['loss'], '#0072bd')
			plt.plot(x, history['val_loss'], '#d95319')
			vl = 0.5
			for v in history['epochs'][:-1]:
				vl += v
				plt.axvline(vl, linestyle='--', lw=0.5, color='#edb120')
				
			plt.xticks(x)
			plt.title('model mean squared error loss')
			plt.ylabel('mean squared error loss')
			plt.xlabel('epoch')
			plt.legend(['training set', 'validation set', 'sessions'], loc='upper right')
			plt.show()
	
if __name__ == "__main__":
    main()